[
["index.html", "Machine Learning Engineer Interview Introduction", " Machine Learning Engineer Interview Panagiotis Katsaroumpas June 21, 2020 Introduction These are my personal notes for interview questions and answers for the role of machine learning engineer. "],
["machine-learning.html", "Chapter 1 Machine Learning", " Chapter 1 Machine Learning How can you reduce overfitting? Collect more data to train with Use data augmentation to generate more train data Use cross-validation to better estimate your error during model selection Reduce the complexity of the model by reducing the number of parameters/layers Use a different model/architecture that generalises better Use a regularisation technique (L1/L2 regularisation, dropout) Use early stopping during training Filter out irrelevant or redundant features using a feature selection technique Use Ensembling to combine predictions from multiple models Define accuracy, precision, recall, specificity, false positive rate Accuracy \\[ \\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} \\] Precision \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\] Recall (sensitivity, true positive rate) \\[ \\text{Recall} = \\frac{TP}{TP + FN} \\] Specificity (selectivitiy, true negative rate) \\[ \\text{Specificity} = \\frac{TN}{FP + TN} \\] False positive rate \\[ \\text{False positive rate} = \\frac{FP}{FP + TN} \\] Define F1 score The F1 score is defined as \\[ F_1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}} \\] Alternative ways to memorise this formula \\[ F_1 = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}} \\] or \\[ F_1 = \\frac{\\text{precision} \\cdot \\text{recall}}{\\frac{\\text{precision} + \\text{recall}}{2}} \\] In terms of true/false positives/negatives it is defined as \\[ F_1 = \\frac{TP}{TP + \\frac{FP + FN}{2}} \\] What are training, validation (development) and test sets? Train (training) set is a set of examples on which the model is trained. Validation set or dev (development) set is a set of examples used for model selection and hyperparameter tuning. This is independent of the training set but it should come from the same distribution. The examples in this set are different from the ones seen by the models during training. Different trained models are all evaluated on the validation/dev set and the best performing model is chosen. Test set is a set of examples that are used to assess the performance of the very final model on examples that have not used before either during training or model selection/hyperparameter tuning. What is the right split for train/dev/test sets? For smaller datasets of up to 10K examples, the split is classically 60% (train) - 20% (dev) - 20% (test). For big datasets that typically contain millions of examples (used in training complex deep learning models), one can afford to do a split of 98% (train) - 1% (dev) - 1% (test). This maximises the amount of data needed for training the complex models, while still setting asside at least 10K of examples for each of the dev and test sets. Define loss, cost and objective functions Loss function is a function defined on a single example producing a single penalty term. It is a measure of error that tells us how good the prediction is compared to the actual label of the example. Cost function is evaluated on all the examples in a specific set (e.g. the train set for train error, the test set for test error, a mini-batch in mini-batch gradient descent), and it is typically a sum (or average) of the loss function over all examples. It might include additional penalty terms like regularisation terms. The ultimate goal of training a model is to minimise the cost function. Objective function is a function that measures the quality of a solution to a problem. One searches for optimal solutions to the problem by optimising the objective function, i.e. finding the parameters of the solution/model that minimise or maximise (depending on the problem) the objective function. The cost function is an example of an objective function. How batch, minibatch and stochastic gradient descent differ? For all algorithms, at each step the parameters of the model are updated based on the gradient of the cost function with respect to the parameters. In (batch) gradient descent, the gradient used in each step is the gradient of the whole cost function which contains the loss function for all examples. Therefore, we need to make a pass (called training epoch) over the whole dataset before each step is taken. In stochastic gradient descent, the gradient used in each step is the gradient for a randomly chosen example. The examples are shuffled and then iterated over. Several passes are made over the examples until the algorithm converges to a minimum of the cost function. In minibatch gradient descent, the data are split into mini-batches of size \\(n\\). The gradient at each step is calculated for a mini-batch of n examples. All examples are iterated over in mini-batches. For \\(n=1\\) we get stochastic gradient descent. For \\(n\\) equal to the total number of examples we get batch gradient descent. Place bayes/human error, training error and dev error on this graph. Define avoidable bias and variance on the same graph. Due to some degree of overfitting the train error is usually lower than the dev error. Assuming that for the specific domain the human error is a good proxy for the Bayes error, then this will be the lowest error possible and therefore the train error would be higher than that. Avoidable bias is the difference between the human error and the train error. Variance is the difference between the train error and the dev error. Will you focus on reducing bias or variance in a neural network in the following cases and how? Case 1 Case 2 Human error 2% 9.5% Train error 10% 10% Dev error 12% 12% In case 1, the focus should be on reducing the bias. This can be achieved by training the network for longer, using a better optimisation algorithm (for example RMSprop or Adam), using a bigger network (more nodes per layer, more layers), changing to a different architecture that is better suited to the task. In case 2, the focus should be on reducing the variance. This can be achieved by training with more data (either by collecting them, or by using data augmentation), using a regularisation techinque (for example dropout layers), changing to a different architecture that is better suited to the task. What is vectorization of an algorithm and what are its benefits Vectorization is the process of removing explicit for loops in order to make an algorithm run more efficiently. This is achieved by grouping relevant variables and representing them with a single matrix rewriting the steps of the algorithm as expressions of these matrices with matrix operations among them implementing these steps/expressions using a library that has vectorized implementations for the matrix operations (for example NumPy). For example, in the formula below, we have a set of weights \\(w_i\\), features \\(x_i\\) and a bias \\(b\\), and in order to calculate the output \\(z\\) we could do an explicit for loop in code for each value of \\(i\\) \\[ z = \\sum_{i=1}^{n} w_i x_i + b. \\] In a vectorized version of this calculation, we group the weights in a column vector \\(W\\) and the features in a column vector \\(X\\) and the sum term can be replaced by the matrix multiplication of the transpose of the weights \\(W^T\\) with the \\(X\\) \\[ z = W^T X + b. \\] Taking this to the next level, we can group the features of all examples into a single matrix where each column contains the features of a single example. With similar matrix multiplication as above we can calculate the output for all examples in the dataset in a single statement (as a column vector \\(Z\\)), avoiding a double nested loop. In some cases for loops might be unavoidable, if one step needs the output of another step, for example as we progress from layer to layer in a deep neural network, or from epoch to epoch. Vectorization can be utilised in various parts of an algorithm. For example, the calculation of the activations during a forward pass in a deep neural network, calculation of the cost function over the current batch of examples, and the calculation of derivatives during a backward pass. Vectorized code runs much faster because it leverages SIMD instructions to paralellise the execution of the code on CPUs and GPUs. This becomes especially critical for Big Data. What are the differences of image classification, object detection, semantic segmentation and instance segmentation? All these are techniques in computer vision. Image classification detects the presence or not of a specific class of an object in a whole picture. It doesn’t tell us in which part of the picture or how many objects are present. Object detection detects one or several objects in an image. It specifies the class of each object, and in addition, it tells us where it is found on the picture by placing a bounding box around it. Semantic segmentation detects the object class of each individual pixel in an image. It produces a mask where the label of each pixel gives us the class of the object it belongs too. This allow us to see boundaries between different types of objects, but it does not produce boundaries between objects of the same class. For example, it tells where parked cars stop and background trees start, but it doesn’t give us boundaries between individual cars. Instance segmentation works at pixel level too similarly to semantic segmentation. In addition, it separates different instances of the same class. This allow us to see boundaries between different instances of objects of the same class. For example, in the output mask pixels belonging to different cars will have different colours.For example, in the output mask pixels belonging to different cars will have different colours. What is the difference among normalizing, scaling and standardizing when performing feature engineering? Normalization acts at the sample (row) level. Each sample is rescaled differently. The components of each sample are rescaled so that its norm equals to one. Normalisation is used in clustering for instance. Scaling acts at the feature (column) level. It scales the feature in the same way for all samples, given an algorithm that takes into account the values of the feature across all samples. Examples of scaling include MinMax scaling (scale to a given range), Mean/variance standardization (see below) and Robust scaling (scaling using statistics that are robust to outliers). Standardization is a type of scaling where we remove the mean of the feature and scale it so that its variance becomes one. Transform the following dataset employing one-hot encoding sample_id phase weight 1 gas 10 2 liquid 20 3 gas 30 4 solid 10 We can only use one-hot encoding on the categorical (discrete) feature phase. We create a binary column for each of its possible values (gas, liquid and solid). The transformed dataset is shown below. sample_id phase_gas phase_liquid phase_solid weight 1 1 0 0 10 2 0 1 0 20 3 1 0 0 30 4 0 0 1 10 "],
["big-data.html", "Chapter 2 Big Data 2.1 Kafka", " Chapter 2 Big Data 2.1 Kafka What ordering guarantee does Kafka provide and how can it be used? Kafka provides total order over records within a partition, while there are no guarantees for records spread accross different partitions. This guarantee is sufficient in many applications and can be leveraged by partitioning the records by the right key. For example, if records are partitioned by the user id, then ordering is preserved for all records for a specific user. As each partition is consumed by a single consumer, this guarantees that records for a specific user will be processed in the same order as they arrived. What is a consumer group? Consumer group is a group of application nodes that splits up the work of consuming messages. This provides the necessary parallelism in order to scale the application to high number of records. It is achieved by dividing up the partitions between the nodes so that every consumer gets a fair share of partitions. Membership in a consumer group is handled dynamically by the Kafka protocol. What is the maximum number of consumers that can consume in parallel from a given topic? As only one consumer can consume messages from a given partition, the maximum number of consumers is equal to the number of partitions for the topic. Extra consumers will be idle, but in some cases this might be desirable for failover. Explain event, ingestion and processing time and place them in chronological order Event time is a timestamp stored in the event itself and it is generated at the source of the event. Ingestion time is the time when the event was stored on the topic. This timestamp is generated by the kafka broker that appends the event to a partition of the topic. Processing time is the time when the event was processed by the application, usually a streaming application. Assuming all clocks are accurate, the chronological order is \\[ \\text{Event time} \\: &lt; \\: \\text{Ingestion time} \\: &lt; \\: \\text{Processing time}. \\] "],
["computer-science.html", "Chapter 3 Computer Science 3.1 Data Structures 3.2 Algorithms 3.3 Functional Programming", " Chapter 3 Computer Science 3.1 Data Structures 3.2 Algorithms 3.3 Functional Programming "],
["programming-languages.html", "Chapter 4 Programming Languages 4.1 Scala 4.2 Python 4.3 SQL", " Chapter 4 Programming Languages 4.1 Scala 4.1.1 What implementations are possible for the following generic function? object GenericFunction { def myFunction[T](input: T): T = ??? } The only possible value of type T we can return is the input itself. So this is the identity function object GenericFunction { def myFunction[T](input: T): T = identity } 4.1.2 A Monad is automatically a Functor. Given the implementation for monad, implement the functor interface. This is the definition for the monad trait Monad[F[_]] { def pure[A](value: A): F[A] def flatMap[A, B](value: F[A])(f: A =&gt; F[B]): F[B] } and the definition for the functor trait Functor[F[_]] { def map[A, B](value: F[A])(f: A =&gt; B): F[B] } Implement the map function given the functions pure and flatMap. trait Monad[F[_]] { def pure[A](value: A): F[A] def flatMap[A, B](value: F[A])(f: A =&gt; F[B]): F[B] def map[A, B](value: F[A])(f: A =&gt; B): F[B] = ??? } The solution is the following def map[A, B](value: F[A])(f: A =&gt; B): F[B] = flatMap(value)(x =&gt; pure(f(x))) 4.2 Python 4.3 SQL "],
["mathematics.html", "Chapter 5 Mathematics 5.1 Linear Algebra 5.2 Probability Theory and Statistics 5.3 Multivariate Calculus", " Chapter 5 Mathematics 5.1 Linear Algebra 5.2 Probability Theory and Statistics State Bayes law Bayes law/theorem/rule states that \\[ P(A|B) = \\frac{P(B|A) \\: P(A)}{P(B)}, \\] where \\(P(X|Y)\\) is the conditional probability of event X occuring given that Y is true. P(A) and P(B) are called marginal probabilities. The law can be easily memorised and derived from the symmetric equation \\[ P(A | B) \\: P(B) = P(B | A)\\: P(A). \\] Explain what a p-value is in the context of an A/B test When performing an A/B test, our null hypothesis is that group B (the variant) is the same as group A (the control) based on some metric. The result of the experiment measures the difference in the value of the metric between the two groups. p-value is the probability of observing such a difference or higher if the null hypothesis is true. In other words, if we did an A/A test instead of an A/B test, the probability of observing at least such an extreme result is this p-value. Explain what Type I and Type II errors are in the context of an A/B test Type I error is an error where we reject the null hypothesis when it is actually true. In an A/B test, this is an error where we conclude that we observed an effect in group B (variant) compared to group A (control), when the effect didn’t really exist. Type II error is an error where we fail to reject the null hypothesis when it is actually false. In an A/B test, this is an error where we fail to observe an effect in group B (variant) compared to group A (control), when the effect did exist. During a survey with a yes/no answer, and in order to provide anonymity, the participants are asked to answer honestly or not based on the outcome of a coin flip. If the outcome is heads, they should give their true answer, if it is tails they should give a positive/yes answer. Would that work? What are the assumptions? If the result of the survey is that 80% of the participants answered positively, what is the true percentage of positive answers? Yes this would work because, assuming the coins used are fair (50% chance of heads or tails) and the number of participants is large we easily calculate the true percentage of positive answers. The flip of the coin affects only the answers of the candidates whose true answer is no/negative. If \\(AY\\) and \\(AN\\) are the percentages of people that answered yes and no respectively, while \\(AY\\) and \\(AN\\) are the true percentages of yes/no response (the response the candidates would have given if they didn’t have to modify their answer), then \\[ AN = p(heads) \\: TN = 0.5 \\: TN, \\] which means that the true percentage of negative answers is \\[ TN = 2 \\:AN = 2 \\cdot 0.2 = 0.4 = 40\\%, \\] and therefore the true percentage of positive answers is \\[ TY = 1 - TN = 1 - 0.4 = 0.6 = 60\\% \\] 5.3 Multivariate Calculus "]
]
