<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Machine Learning | Machine Learning Engineer Interview</title>
  <meta name="description" content="Chapter 1 Machine Learning | Machine Learning Engineer Interview Questions and Answers" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Machine Learning | Machine Learning Engineer Interview" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 1 Machine Learning | Machine Learning Engineer Interview Questions and Answers" />
  <meta name="github-repo" content="codiply/machine-learning-engineer-interview" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Machine Learning | Machine Learning Engineer Interview" />
  
  <meta name="twitter:description" content="Chapter 1 Machine Learning | Machine Learning Engineer Interview Questions and Answers" />
  

<meta name="author" content="Panagiotis Katsaroumpas" />


<meta name="date" content="2020-06-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="big-data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning Engineer Interview</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>1</b> Machine Learning</a><ul>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#how-can-you-reduce-overfitting"><i class="fa fa-check"></i>How can you reduce overfitting?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#define-accuracy-precision-recall-specificity-false-positive-rate"><i class="fa fa-check"></i>Define accuracy, precision, recall, specificity, false positive rate</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#define-f1-score"><i class="fa fa-check"></i>Define F1 score</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#what-are-training-validation-development-and-test-sets"><i class="fa fa-check"></i>What are training, validation (development) and test sets?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#what-is-the-right-split-for-traindevtest-sets"><i class="fa fa-check"></i>What is the right split for train/dev/test sets?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#define-loss-cost-and-objective-functions"><i class="fa fa-check"></i>Define loss, cost and objective functions</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#how-batch-minibatch-and-stochastic-gradient-descent-differ"><i class="fa fa-check"></i>How batch, minibatch and stochastic gradient descent differ?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#place-bayeshuman-error-training-error-and-dev-error-on-this-graph.-define-avoidable-bias-and-variance-on-the-same-graph."><i class="fa fa-check"></i>Place bayes/human error, training error and dev error on this graph. Define avoidable bias and variance on the same graph.</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#will-you-focus-on-reducing-bias-or-variance-in-a-neural-network-in-the-following-cases-and-how"><i class="fa fa-check"></i>Will you focus on reducing bias or variance in a neural network in the following cases and how?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#what-is-vectorization-of-an-algorithm-and-what-are-its-benefits"><i class="fa fa-check"></i>What is vectorization of an algorithm and what are its benefits</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#what-are-the-differences-of-image-classification-object-detection-semantic-segmentation-and-instance-segmentation"><i class="fa fa-check"></i>What are the differences of image classification, object detection, semantic segmentation and instance segmentation?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#what-is-the-difference-among-normalizing-scaling-and-standardizing-when-performing-feature-engineering"><i class="fa fa-check"></i>What is the difference among normalizing, scaling and standardizing when performing feature engineering?</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#transform-the-following-dataset-employing-one-hot-encoding"><i class="fa fa-check"></i>Transform the following dataset employing one-hot encoding</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="big-data.html"><a href="big-data.html"><i class="fa fa-check"></i><b>2</b> Big Data</a><ul>
<li class="chapter" data-level="2.1" data-path="big-data.html"><a href="big-data.html#kafka"><i class="fa fa-check"></i><b>2.1</b> Kafka</a><ul>
<li class="chapter" data-level="" data-path="big-data.html"><a href="big-data.html#what-ordering-guarantee-does-kafka-provide-and-how-can-it-be-used"><i class="fa fa-check"></i>What ordering guarantee does Kafka provide and how can it be used?</a></li>
<li class="chapter" data-level="" data-path="big-data.html"><a href="big-data.html#what-is-a-consumer-group"><i class="fa fa-check"></i>What is a consumer group?</a></li>
<li class="chapter" data-level="" data-path="big-data.html"><a href="big-data.html#what-is-the-maximum-number-of-consumers-that-can-consume-in-parallel-from-a-given-topic"><i class="fa fa-check"></i>What is the maximum number of consumers that can consume in parallel from a given topic?</a></li>
<li class="chapter" data-level="" data-path="big-data.html"><a href="big-data.html#explain-event-ingestion-and-processing-time-and-place-them-in-chronological-order"><i class="fa fa-check"></i>Explain event, ingestion and processing time and place them in chronological order</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="computer-science.html"><a href="computer-science.html"><i class="fa fa-check"></i><b>3</b> Computer Science</a><ul>
<li class="chapter" data-level="3.1" data-path="computer-science.html"><a href="computer-science.html#data-structures"><i class="fa fa-check"></i><b>3.1</b> Data Structures</a></li>
<li class="chapter" data-level="3.2" data-path="computer-science.html"><a href="computer-science.html#algorithms"><i class="fa fa-check"></i><b>3.2</b> Algorithms</a></li>
<li class="chapter" data-level="3.3" data-path="computer-science.html"><a href="computer-science.html#functional-programming"><i class="fa fa-check"></i><b>3.3</b> Functional Programming</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="programming-languages.html"><a href="programming-languages.html"><i class="fa fa-check"></i><b>4</b> Programming Languages</a><ul>
<li class="chapter" data-level="4.1" data-path="programming-languages.html"><a href="programming-languages.html#scala"><i class="fa fa-check"></i><b>4.1</b> Scala</a><ul>
<li class="chapter" data-level="4.1.1" data-path="programming-languages.html"><a href="programming-languages.html#what-implementations-are-possible-for-the-following-generic-function"><i class="fa fa-check"></i><b>4.1.1</b> What implementations are possible for the following generic function?</a></li>
<li class="chapter" data-level="4.1.2" data-path="programming-languages.html"><a href="programming-languages.html#a-monad-is-automatically-a-functor.-given-the-implementation-for-monad-implement-the-functor-interface."><i class="fa fa-check"></i><b>4.1.2</b> A Monad is automatically a Functor. Given the implementation for monad, implement the functor interface.</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="programming-languages.html"><a href="programming-languages.html#python"><i class="fa fa-check"></i><b>4.2</b> Python</a></li>
<li class="chapter" data-level="4.3" data-path="programming-languages.html"><a href="programming-languages.html#sql"><i class="fa fa-check"></i><b>4.3</b> SQL</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>5</b> Mathematics</a><ul>
<li class="chapter" data-level="5.1" data-path="mathematics.html"><a href="mathematics.html#linear-algebra"><i class="fa fa-check"></i><b>5.1</b> Linear Algebra</a></li>
<li class="chapter" data-level="5.2" data-path="mathematics.html"><a href="mathematics.html#probability-theory-and-statistics"><i class="fa fa-check"></i><b>5.2</b> Probability Theory and Statistics</a><ul>
<li class="chapter" data-level="" data-path="mathematics.html"><a href="mathematics.html#state-bayes-law"><i class="fa fa-check"></i>State Bayes law</a></li>
<li class="chapter" data-level="" data-path="mathematics.html"><a href="mathematics.html#explain-what-a-p-value-is-in-the-context-of-an-ab-test"><i class="fa fa-check"></i>Explain what a p-value is in the context of an A/B test</a></li>
<li class="chapter" data-level="" data-path="mathematics.html"><a href="mathematics.html#explain-what-type-i-and-type-ii-errors-are-in-the-context-of-an-ab-test"><i class="fa fa-check"></i>Explain what Type I and Type II errors are in the context of an A/B test</a></li>
<li class="chapter" data-level="" data-path="mathematics.html"><a href="mathematics.html#during-a-survey-with-a-yesno-answer-and-in-order-to-provide-anonymity-the-participants-are-asked-to-answer-honestly-or-not-based-on-the-outcome-of-a-coin-flip.-if-the-outcome-is-heads-they-should-give-their-true-answer-if-it-is-tails-they-should-give-a-positiveyes-answer.-would-that-work-what-are-the-assumptions-if-the-result-of-the-survey-is-that-80-of-the-participants-answered-positively-what-is-the-true-percentage-of-positive-answers"><i class="fa fa-check"></i>During a survey with a yes/no answer, and in order to provide anonymity, the participants are asked to answer honestly or not based on the outcome of a coin flip. If the outcome is heads, they should give their true answer, if it is tails they should give a positive/yes answer. Would that work? What are the assumptions? If the result of the survey is that 80% of the participants answered positively, what is the true percentage of positive answers?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mathematics.html"><a href="mathematics.html#multivariate-calculus"><i class="fa fa-check"></i><b>5.3</b> Multivariate Calculus</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Engineer Interview</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Machine Learning</h1>
<div id="how-can-you-reduce-overfitting" class="section level3 unnumbered">
<h3>How can you reduce overfitting?</h3>
<ul>
<li>Collect more data to train with</li>
<li>Use data augmentation to generate more train data</li>
<li>Use cross-validation to better estimate your error during model selection</li>
<li>Reduce the complexity of the model by reducing the number of parameters/layers</li>
<li>Use a different model/architecture that generalises better</li>
<li>Use a regularisation technique (L1/L2 regularisation, dropout)</li>
<li>Use early stopping during training</li>
<li>Filter out irrelevant or redundant features using a feature selection technique</li>
<li>Use Ensembling to combine predictions from multiple models</li>
</ul>
</div>
<div id="define-accuracy-precision-recall-specificity-false-positive-rate" class="section level3 unnumbered">
<h3>Define accuracy, precision, recall, specificity, false positive rate</h3>
<div id="accuracy" class="section level4 unnumbered">
<h4>Accuracy</h4>
<p><span class="math display">\[
  \text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}
\]</span></p>
<p><img src="images/accuracy-definition-diagram.svg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="precision" class="section level4 unnumbered">
<h4>Precision</h4>
<p><span class="math display">\[
  \text{Precision} = \frac{TP}{TP + FP}
\]</span></p>
<p><img src="images/precision-definition-diagram.svg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="recall-sensitivity-true-positive-rate" class="section level4 unnumbered">
<h4>Recall (sensitivity, true positive rate)</h4>
<p><span class="math display">\[
  \text{Recall} = \frac{TP}{TP + FN}
\]</span></p>
<p><img src="images/recall-definition-diagram.svg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="specificity-selectivitiy-true-negative-rate" class="section level4 unnumbered">
<h4>Specificity (selectivitiy, true negative rate)</h4>
<p><span class="math display">\[
  \text{Specificity} = \frac{TN}{FP + TN}
\]</span></p>
<p><img src="images/specificity-definition-diagram.svg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="false-positive-rate" class="section level4 unnumbered">
<h4>False positive rate</h4>
<p><span class="math display">\[
  \text{False positive rate} = \frac{FP}{FP + TN}
\]</span></p>
<p><img src="images/false-positive-rate-definition-diagram.svg" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="define-f1-score" class="section level3 unnumbered">
<h3>Define F1 score</h3>
<p>The F1 score is defined as</p>
<p><span class="math display">\[
  F_1 = 2 \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]</span>
Alternative ways to memorise this formula</p>
<p><span class="math display">\[
  F_1 = \frac{2}{\frac{1}{\text{precision}} + \frac{1}{\text{recall}}}
\]</span>
or</p>
<p><span class="math display">\[
  F_1 =  \frac{\text{precision} \cdot \text{recall}}{\frac{\text{precision} + \text{recall}}{2}}
\]</span></p>
<p>In terms of true/false positives/negatives it is defined as</p>
<p><span class="math display">\[
  F_1 =  \frac{TP}{TP + \frac{FP + FN}{2}}
\]</span></p>
</div>
<div id="what-are-training-validation-development-and-test-sets" class="section level3 unnumbered">
<h3>What are training, validation (development) and test sets?</h3>
<p><strong>Train (training) set</strong> is a set of examples on which the model is trained.</p>
<p><strong>Validation set</strong> or <strong>dev (development) set</strong> is a set of examples used for model selection and hyperparameter tuning. This is independent of the training set but it should come from the same distribution. The examples in this set are different from the ones seen by the models during training. Different trained models are all evaluated on the validation/dev set and the best performing model is chosen.</p>
<p><strong>Test set</strong> is a set of examples that are used to assess the performance of the very final model on examples that have not used before either during training or model selection/hyperparameter tuning.</p>
</div>
<div id="what-is-the-right-split-for-traindevtest-sets" class="section level3 unnumbered">
<h3>What is the right split for train/dev/test sets?</h3>
<ul>
<li>For smaller datasets of up to 10K examples, the split is classically <strong>60% (train) - 20% (dev) - 20% (test)</strong>.</li>
<li>For big datasets that typically contain millions of examples (used in training complex deep learning models), one can afford to do a split of <strong>98% (train) - 1% (dev) - 1% (test)</strong>. This maximises the amount of data needed for training the complex models, while still setting asside at least 10K of examples for each of the dev and test sets.</li>
</ul>
</div>
<div id="define-loss-cost-and-objective-functions" class="section level3 unnumbered">
<h3>Define loss, cost and objective functions</h3>
<ul>
<li><p>Loss function is a function defined on a single example producing a single penalty term. It is a measure of error that tells us how good the prediction is compared to the actual label of the example.</p></li>
<li><p>Cost function is evaluated on all the examples in a specific set (e.g. the train set for train error, the test set for test error, a mini-batch in mini-batch gradient descent), and it is typically a sum (or average) of the loss function over all examples. It might include additional penalty terms like regularisation terms. The ultimate goal of training a model is to minimise the cost function.</p></li>
<li><p>Objective function is a function that measures the quality of a solution to a problem. One searches for optimal solutions to the problem by optimising the objective function, i.e. finding the parameters of the solution/model that minimise or maximise (depending on the problem) the objective function. The cost function is an example of an objective function.</p></li>
</ul>
</div>
<div id="how-batch-minibatch-and-stochastic-gradient-descent-differ" class="section level3 unnumbered">
<h3>How batch, minibatch and stochastic gradient descent differ?</h3>
<p>For all algorithms, at each step the parameters of the model are updated based on the gradient of the cost function with respect to the parameters.</p>
<ul>
<li>In (batch) gradient descent, the gradient used in each step is the gradient of the whole cost function which contains the loss function for all examples. Therefore, we need to make a pass (called training epoch) over the whole dataset before each step is taken.</li>
<li>In stochastic gradient descent, the gradient used in each step is the gradient for a randomly chosen example. The examples are shuffled and then iterated over. Several passes are made over the examples until the algorithm converges to a minimum of the cost function.</li>
<li>In minibatch gradient descent, the data are split into mini-batches of size <span class="math inline">\(n\)</span>. The gradient at each step is calculated for a mini-batch of n examples. All examples are iterated over in mini-batches. For <span class="math inline">\(n=1\)</span> we get stochastic gradient descent. For <span class="math inline">\(n\)</span> equal to the total number of examples we get batch gradient descent.</li>
</ul>
</div>
<div id="place-bayeshuman-error-training-error-and-dev-error-on-this-graph.-define-avoidable-bias-and-variance-on-the-same-graph." class="section level3 unnumbered">
<h3>Place bayes/human error, training error and dev error on this graph. Define avoidable bias and variance on the same graph.</h3>
<p><img src="images/avoidable-bias-variance-definition-empty-graph.svg" width="70%" style="display: block; margin: auto;" /></p>
<p>Due to some degree of overfitting the train error is usually lower than the dev error. Assuming that for the specific domain the human error is a good proxy for the Bayes error, then this will be the lowest error possible and therefore the train error would be higher than that.</p>
<p><img src="images/avoidable-bias-variance-definition.svg" width="70%" style="display: block; margin: auto;" /></p>
<p>Avoidable bias is the difference between the human error and the train error. Variance is the difference between the train error and the dev error.</p>
</div>
<div id="will-you-focus-on-reducing-bias-or-variance-in-a-neural-network-in-the-following-cases-and-how" class="section level3 unnumbered">
<h3>Will you focus on reducing bias or variance in a neural network in the following cases and how?</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Case 1</th>
<th align="center">Case 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Human error</td>
<td align="center">2%</td>
<td align="center">9.5%</td>
</tr>
<tr class="even">
<td>Train error</td>
<td align="center">10%</td>
<td align="center">10%</td>
</tr>
<tr class="odd">
<td>Dev error</td>
<td align="center">12%</td>
<td align="center">12%</td>
</tr>
</tbody>
</table>
<p>In case 1, the focus should be on reducing the bias. This can be achieved by</p>
<ul>
<li>training the network for longer,</li>
<li>using a better optimisation algorithm (for example RMSprop or Adam),</li>
<li>using a bigger network (more nodes per layer, more layers),</li>
<li>changing to a different architecture that is better suited to the task.</li>
</ul>
<p>In case 2, the focus should be on reducing the variance. This can be achieved by</p>
<ul>
<li>training with more data (either by collecting them, or by using data augmentation),</li>
<li>using a regularisation techinque (for example dropout layers),</li>
<li>changing to a different architecture that is better suited to the task.</li>
</ul>
</div>
<div id="what-is-vectorization-of-an-algorithm-and-what-are-its-benefits" class="section level3 unnumbered">
<h3>What is vectorization of an algorithm and what are its benefits</h3>
<p>Vectorization is the process of removing explicit <em>for loops</em> in order to make an algorithm run more efficiently. This is achieved by</p>
<ul>
<li>grouping relevant variables and representing them with a single matrix</li>
<li>rewriting the steps of the algorithm as expressions of these matrices with matrix operations among them</li>
<li>implementing these steps/expressions using a library that has vectorized implementations for the matrix operations (for example NumPy).</li>
</ul>
<p>For example, in the formula below, we have a set of weights <span class="math inline">\(w_i\)</span>, features <span class="math inline">\(x_i\)</span> and a bias <span class="math inline">\(b\)</span>, and in order to calculate the output <span class="math inline">\(z\)</span> we could do an explicit for loop in code for each value of <span class="math inline">\(i\)</span></p>
<p><span class="math display">\[
  z = \sum_{i=1}^{n} w_i x_i + b.
\]</span></p>
<p>In a vectorized version of this calculation, we group the weights in a column vector <span class="math inline">\(W\)</span> and the features in a column vector <span class="math inline">\(X\)</span> and the sum term can be replaced by the matrix multiplication of the transpose of the weights <span class="math inline">\(W^T\)</span> with the <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
  z = W^T X + b.
\]</span></p>
<p>Taking this to the next level, we can group the features of all examples into a single matrix where each column contains the features of a single example. With similar matrix multiplication as above we can calculate the output for all examples in the dataset in a single statement (as a column vector <span class="math inline">\(Z\)</span>), avoiding a double nested loop. In some cases for loops might be unavoidable, if one step needs the output of another step, for example as we progress from layer to layer in a deep neural network, or from epoch to epoch.</p>
<p>Vectorization can be utilised in various parts of an algorithm. For example, the calculation of the activations during a forward pass in a deep neural network, calculation of the cost function over the current batch of examples, and the calculation of derivatives during a backward pass.</p>
<p>Vectorized code runs much faster because it leverages <a href="https://en.wikipedia.org/wiki/SIMD">SIMD</a> instructions to paralellise the execution of the code on CPUs and GPUs. This becomes especially critical for Big Data.</p>
</div>
<div id="what-are-the-differences-of-image-classification-object-detection-semantic-segmentation-and-instance-segmentation" class="section level3 unnumbered">
<h3>What are the differences of image classification, object detection, semantic segmentation and instance segmentation?</h3>
<p>All these are techniques in computer vision.</p>
<ul>
<li>Image classification detects the presence or not of a specific class of an object in a whole picture.
It doesn’t tell us in which part of the picture or how many objects are present.</li>
<li>Object detection detects one or several objects in an image. It specifies the class of each object,
and in addition, it tells us where it is found on the picture by placing a bounding box around it.</li>
<li>Semantic segmentation detects the object class of each individual pixel in an image.
It produces a mask where the label of each pixel gives us the class of the object it belongs too.
This allow us to see boundaries between different types of objects, but it does not produce boundaries between
objects of the same class. For example, it tells where parked cars stop and background trees start, but
it doesn’t give us boundaries between individual cars.</li>
<li>Instance segmentation works at pixel level too similarly to semantic segmentation. In addition,
it separates different instances of the same class.
This allow us to see boundaries between different instances of objects of the same class.
For example, in the output mask pixels belonging to different cars will have different colours.For example, in the output mask pixels belonging to different cars will have different colours.</li>
</ul>
</div>
<div id="what-is-the-difference-among-normalizing-scaling-and-standardizing-when-performing-feature-engineering" class="section level3 unnumbered">
<h3>What is the difference among normalizing, scaling and standardizing when performing feature engineering?</h3>
<ul>
<li>Normalization acts at the sample (row) level. Each sample is rescaled differently.
The components of each sample are rescaled so that its norm equals to one. Normalisation is used in clustering for instance.</li>
<li>Scaling acts at the feature (column) level. It scales the feature in the same way for all samples,
given an algorithm that takes into account the values of the feature across all samples.
Examples of scaling include MinMax scaling (scale to a given range), Mean/variance standardization (see below) and
Robust scaling (scaling using statistics that are robust to outliers).</li>
<li>Standardization is a type of scaling where we remove the mean of the feature and scale it so that its variance becomes one.</li>
</ul>
</div>
<div id="transform-the-following-dataset-employing-one-hot-encoding" class="section level3 unnumbered">
<h3>Transform the following dataset employing one-hot encoding</h3>
<table>
<thead>
<tr class="header">
<th align="center">sample_id</th>
<th align="center">phase</th>
<th align="center">weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">gas</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">liquid</td>
<td align="center">20</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">gas</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">solid</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>We can only use one-hot encoding on the categorical (discrete) feature <em>phase</em>. We create a binary column for each of
its possible values (gas, liquid and solid). The transformed dataset is shown below.</p>
<table>
<thead>
<tr class="header">
<th align="center">sample_id</th>
<th align="center">phase_gas</th>
<th align="center">phase_liquid</th>
<th align="center">phase_solid</th>
<th align="center">weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">20</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">10</td>
</tr>
</tbody>
</table>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="big-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/codiply/machine-learning-engineer-interview/edit/master/book/01-machine-learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
